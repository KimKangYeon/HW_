{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "중간고사 1번째 파일",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimKangYeon/HW_/blob/master/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC_1%EB%B2%88%EC%A7%B8_%ED%8C%8C%EC%9D%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdXUYXXvlVjt"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from abc import *\n",
        "import sys\n",
        "import tqdm\n",
        "import joblib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w20DLnyklfeA",
        "outputId": "345a9fb2-3b9f-411c-9ca6-bc61f6ad2a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, 10)\n",
        "y_test_encoded = to_categorical(y_test, 10)\n",
        "\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1,784)\n",
        "\n",
        "x_val = x_train[54000:]\n",
        "y_val_encoded = y_train_encoded[54000:]\n",
        "x_train = x_train[:54000]\n",
        "y_train_encoded = y_train_encoded[:54000]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFeya5TRlgut"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1+np.exp(-x))\n",
        "\n",
        "  \n",
        "def softmax(array):\n",
        "  return np.exp(array) / np.sum(np.exp(array), axis = 1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouzbHuqtljmk"
      },
      "source": [
        "class layer():\n",
        "  '''\n",
        "  layer class. Dense층을 형성합니다.\n",
        "  '''\n",
        "  def __init__(self, n_neuron):\n",
        "    self.n_neuron = n_neuron"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzenfQk4lpCg"
      },
      "source": [
        "class FCNet():\n",
        "  '''\n",
        "  Fully Connected net 을 생성합니다.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "    super().__init__()\n",
        "\n",
        "  def append_layer(self, n_neuron):\n",
        "    self.layers.append(layer(n_neuron)) #layer object들을 추가합니다. layer object는 n_neuron을 그 속성으로 가집니다. 따라서, model.append_layer(300)은 뉴런의 개수가 300개인 레이어가 모델에 추가됩니다.\n",
        "\n",
        "  def compile(self, weight_mode = 'normal'):\n",
        "    if weight_mode not in ['ones', 'zeros', 'normal', 'uniform']:\n",
        "      print(\"weight mode should be one of the 'ones', 'zeros', 'normal', 'uniform'\")\n",
        "      sys.exit()\n",
        "    self.weight = [] # 각 층의 weight를 담을 리스트를 생성합니다.\n",
        "    self.bias = [] # 각 층의 bias를 담을 리스트를 생성합니다.\n",
        "    self.len_layers = len(self.layers) # layer의 개수를 의미합니다. \n",
        "    for i in range(self.len_layers - 1): # weight, bias는 각 층의 연결시에 필요하므로 그 개수는 레이어 개수보다 한 개 적습니다.\n",
        "      if weight_mode == 'normal':\n",
        "        W = np.random.normal(0, 1, (self.layers[i].n_neuron, self.layers[i+1].n_neuron)) # 각 층의 weight는 [입력층 뉴런 수, 출력층 뉴런 수]의 파라미터를 가집니다.\n",
        "        b = np.random.normal(0, 1, (self.layers[i+1].n_neuron)) # 각 층의 bias는 [출력층 뉴런수]개의 파라미터를 가집니다.\n",
        "      if weight_mode == 'uniform':\n",
        "        W = np.random.uniform(0, 1, (self.layers[i].n_neuron, self.layers[i+1].n_neuron)) # 각 층의 weight는 [입력층 뉴런 수, 출력층 뉴런 수]의 파라미터를 가집니다.\n",
        "        b = np.random.uniform(0, 1, (self.layers[i+1].n_neuron)) # 각 층의 bias는 [출력층 뉴런수]개의 파라미터를 가집니다.\n",
        "      if weight_mode == 'zeros':\n",
        "        W = np.zeros((self.layers[i].n_neuron, self.layers[i+1].n_neuron)) # 각 층의 weight는 [입력층 뉴런 수, 출력층 뉴런 수]의 파라미터를 가집니다.\n",
        "        b = np.zeros((self.layers[i+1].n_neuron)) # 각 층의 bias는 [출력층 뉴런수]개의 파라미터를 가집니다.\n",
        "      if weight_mode == 'ones':\n",
        "        W = np.ones((self.layers[i].n_neuron, self.layers[i+1].n_neuron)) # 각 층의 weight는 [입력층 뉴런 수, 출력층 뉴런 수]의 파라미터를 가집니다.\n",
        "        b = np.ones((self.layers[i+1].n_neuron)) # 각 층의 bias는 [출력층 뉴런수]개의 파라미터를 가집니다.\n",
        "      self.weight.append(W) # 리스트에 weight를 저장합니다.\n",
        "      self.bias.append(b)\n",
        "    self.compiled = True\n",
        "\n",
        "  def forward(self, batch):\n",
        "    self.layer_outputs = [batch] # layer_outputs를 생성합니다. 이는 각 출력층의 값들입니다. 첫 번째 원소는 input이여야 하므로, 입력받은 batch를 이미 리스트로 가지고 있습니다.\n",
        "    for index, (weight, bias) in enumerate(zip(self.weight, self.bias)): # enumerate 함수는 index, object를, zip 함수는 두 object를 동시에 호출하는 함수입니다.\n",
        "      new_batch = np.dot(batch, weight) + bias # 다음 층의 출력을 계산합니다. Wx + b\n",
        "      new_batch = sigmoid(new_batch) # activation 함수를 통과시킵니다.\n",
        "      if index == self.len_layers - 1: # 마지막 층에서는 \n",
        "        new_batch = softmax(new_batch) # softmax함수에 통과시킵니다.(전체 확률은 1이 됩니다.)\n",
        "      batch = new_batch # 다음 층에도 이를 통과시키기 위해 batch를 new_batch로 변경합니다.\n",
        "      self.layer_outputs.append(batch) # layer_outputs에 출력층의 값을 추가해줍니다.\n",
        "\n",
        "  def loss(self, prediction, output): # Cross entropy 값을 loss로 갖는 함수입니다.\n",
        "    cross_entropy = - (np.sum(output * np.log(prediction) + (1-output) * np.log(1-prediction))) / len(output)\n",
        "    return cross_entropy\n",
        "\n",
        "  def backpropagation(self, output): \n",
        "    self.weight_grad = [] # weight의 기울기 리스트를 생성합니다.\n",
        "    self.bias_grad = []# bias의 기울기 리스트를 생성합니다.\n",
        "    for idx in range(self.len_layers - 1): # 인덱싱이 복잡합니다. 역전파 이므로 순서가 반대입니다.\n",
        "      if idx == 0: # 마지막 층 인덱스에서\n",
        "        dC_ds = self.layer_outputs[-1] - output # dC/ds 는 (x_k - t_k) 2 * (asdfasdf)\n",
        "      else: # 나머지 층에서\n",
        "        x = self.layer_outputs[-(idx+1)]\n",
        "        dx_ds = x * (1 - x) # dx/ds 는 sigmoid의 미분 x(1-x)입니다.z\n",
        "        dC_ds = dC_ds @ self.weight[-(idx)].T * dx_ds # dC/dsj = dC/dsk * dsk/dxj * dxj/dsj \n",
        "      ds_dw = self.layer_outputs[-(idx+2)] # dsj/dwj는 항상 그 이전층 xi입니다.\n",
        "      dC_dw = (dC_ds.reshape(self.batch_size, -1, 1) @ ds_dw.reshape(self.batch_size, 1,-1)).T #dC_dw = dC/ds * ds/dw\n",
        "      dC_db = dC_ds # ds/db = 1 입니다.\n",
        "      dC_dw = np.mean(dC_dw, axis = 2) # 각 batch의 cost function은 원래 cost function의 평균값입니다.\n",
        "      dC_db = np.mean(dC_db, axis = 0) \n",
        "      self.weight_grad.append(dC_dw) # weight grad 리스트에 weight의 기울기를 추가합니다.\n",
        "      self.bias_grad.append(dC_db)\n",
        "    self.weight_grad = self.weight_grad[::-1] #오차 역전파는 반대방향이므로, 방향을 통일하기 위해 뒤집습니다.\n",
        "    self.bias_grad = self.bias_grad[::-1]\n",
        "  \n",
        "  def accuracy(self, predict, output):\n",
        "    return np.where(np.argmax(predict, axis = 1) == np.argmax(output, axis=1), 1,0).sum() / len(output)\n",
        "\n",
        "  def validation(self, validation_input, validation_output):\n",
        "    self.forward(validation_input)\n",
        "    predict = self.layer_outputs[-1]\n",
        "    acc = self.accuracy(predict, validation_output)\n",
        "    loss = self.loss(predict, validation_output)\n",
        "    print(\"\")\n",
        "    print('val_accuracy = {}'.format(acc))\n",
        "    print('val_loss = {}'.format(loss))\n",
        "\n",
        "  def fit(self, x, y, val_x, val_y, epochs, batch_size, lr = 0.1, lambda_1 = 0.0001, lambda_2 =0.01):\n",
        "    self.batch_size = batch_size\n",
        "    self.val_loss_history = []\n",
        "    self.val_acc_history = []\n",
        "    self.train_loss_history = []\n",
        "    self.train_acc_history = []\n",
        "    self.trainer = True\n",
        "    if self.trainer:\n",
        "      for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        batch_generator = self.batch_generator(x, y, self.batch_size) # batch를 생성할 수 있는 generator를 생성합니다. 제너레이터는 데이터들을 호출합니다.\n",
        "        steps = len(x) // batch_size \n",
        "        for i in tqdm.tqdm(range(steps)): # batch의 개수입니다. tqdm module은 상태 바를 만들어줍니다. 진행 상황을 확인할 수 있습니다.\n",
        "          batch, batch_output = next(batch_generator)\n",
        "          self.forward(batch) # batch를 이용하여 데이터를 예측합니다.\n",
        "          self.backpropagation(batch_output) # 오차 역전파를 진행합니다.\n",
        "          for i in range(self.len_layers - 1): \n",
        "            self.weight[i] = (1 - lr * lambda_2 / len(batch)) * self.weight[i] - lr * self.weight_grad[i] - lr * lambda_1 * np.sign(self.weight[i]) # weight를 업데이트 합니다. weight_grad 값에 스칼라를 곱해 이 속도를 조절할 수 있습니다.\n",
        "            self.bias[i] = self.bias[i] - lr *self.bias_grad[i] # bias를 업데이트 합니다. \n",
        "          train_loss += self.loss(self.layer_outputs[-1], batch_output)\n",
        "          train_acc += self.accuracy(self.layer_outputs[-1], batch_output)\n",
        "        train_loss /= steps\n",
        "        train_acc /= steps\n",
        "        self.train_loss_history.append(train_loss)    \n",
        "        self.train_acc_history.append(train_acc)\n",
        "        print(\"train_loss : {}\".format(train_loss))\n",
        "        print(\"train_acc : {}\".format(train_acc))\n",
        "        self.validation(val_x, val_y)\n",
        "        self.early_stopping(5)\n",
        "\n",
        "\n",
        "  def batch_generator(self, x, y , batch_size): # batch generator를 정의합니다. \n",
        "    i = -1\n",
        "    while True:\n",
        "      i += 1\n",
        "      if (i+2)*batch_size < len(x):\n",
        "        yield x[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]\n",
        "      else:\n",
        "        self.batch_size = len(x[i*batch_size:])\n",
        "        yield x[i*batch_size:], y[i*batch_size:]\n",
        "        i = -1\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "  def validation(self, validation_input, validation_output):\n",
        "    self.forward(validation_input)\n",
        "    predict = self.layer_outputs[-1]\n",
        "    acc = self.accuracy(predict, validation_output)\n",
        "    loss = self.loss(predict, validation_output)\n",
        "    self.val_loss_history.append(loss)\n",
        "    self.val_acc_history.append(acc)\n",
        "    print('\\nval_loss = {}'.format(loss))\n",
        "    print('val_accuracy = {}'.format(acc))\n",
        "\n",
        "  def plot_graph(self):\n",
        "    plt.style.use('ggplot')\n",
        "    mpl.rcParams['axes.unicode_minus'] = False\n",
        "    \n",
        "    fig, axes = plt.subplots(2,1)\n",
        "    axes[0].plot(self.train_loss_history, label='train_loss')\n",
        "    axes[0].plot(self.val_loss_history, label='test_loss')\n",
        "    axes[0].title.set_text(\"loss graph\")\n",
        "    plt.legend()\n",
        "    axes[1].plot(self.train_acc_history, label = 'train_acc')\n",
        "    axes[1].plot(self.val_acc_history, label = 'val_acc')\n",
        "    axes[1].title.set_text(\"acc graph\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  def early_stopping(self, watch):\n",
        "    best_history = np.argmax(self.val_acc_history)\n",
        "    if best_history == len(self.val_acc_history) - watch:\n",
        "      print(\"early stopped.\")\n",
        "      self.trainer = 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHwap4N1lupE"
      },
      "source": [
        "net = FCNet()\n",
        "net.append_layer(784)\n",
        "net.append_layer(256)\n",
        "net.append_layer(10)\n",
        "net.compile(weight_mode = 'normal')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6746ZQr-lvdc"
      },
      "source": [
        "def savemodel(filename, model):        \n",
        "  joblib.dump(model,filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scOXijiclxWs",
        "outputId": "28b3d904-e335-496d-ef0a-02fa4b60191e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net.fit(x_train, y_train_encoded, x_test, y_test_encoded, 30, 64, lr= 0.1)\n",
        "savemodel(\"/content/model_v0\", net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:06<00:00, 12.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 2.607766670559269\n",
            "train_acc : 0.6943870742247077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:48,  7.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 1.3600411624853117\n",
            "val_accuracy = 0.8307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:50<00:00,  7.64it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 1.2045528575918438\n",
            "train_acc : 0.8484102270801553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:48,  7.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 1.0229999959014715\n",
            "val_accuracy = 0.8705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:51<00:00,  7.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.9532415913324918\n",
            "train_acc : 0.8785017899508557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:45,  7.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.8683540870041326\n",
            "val_accuracy = 0.8883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:51<00:00,  7.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.8149224029389704\n",
            "train_acc : 0.8948734854261993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:48,  7.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.77267788175487\n",
            "val_accuracy = 0.9002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:52<00:00,  7.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.7231273615678739\n",
            "train_acc : 0.905916370106762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:50,  7.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.7063356830097172\n",
            "val_accuracy = 0.9085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:52<00:00,  7.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.6564557094745073\n",
            "train_acc : 0.9139420225385531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:49,  7.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.6570113539060903\n",
            "val_accuracy = 0.9135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:52<00:00,  7.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.6053534966001334\n",
            "train_acc : 0.9204875233011356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:51,  7.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.618486639128396\n",
            "val_accuracy = 0.9185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:53<00:00,  7.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.5647145351553772\n",
            "train_acc : 0.9253489874597518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:50,  7.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.5872566515705538\n",
            "val_accuracy = 0.9202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:53<00:00,  7.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.531528530484946\n",
            "train_acc : 0.9300184820369427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:52,  7.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.5611833868699727\n",
            "val_accuracy = 0.9234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:57<00:00,  7.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.50380836335795\n",
            "train_acc : 0.9335983837485172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:54,  7.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.5389894647696574\n",
            "val_accuracy = 0.9253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:58<00:00,  7.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.480241272281749\n",
            "train_acc : 0.9367096148957806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:50,  7.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.5198566287492308\n",
            "val_accuracy = 0.9278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 843/843 [01:56<00:00,  7.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss : 0.4599571689711089\n",
            "train_acc : 0.9396116653956955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/843 [00:00<01:50,  7.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_loss = 0.5030875288390331\n",
            "val_accuracy = 0.931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 282/843 [00:38<01:14,  7.49it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94mtYw7xlyR8"
      },
      "source": [
        "savemodel(\"/content/model_v0\", net)\n",
        "\n",
        "net.plot_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHxq2tlMrL3U"
      },
      "source": [
        "epoch /batch_size/ lr/ lamda_1/ lamda_2\n",
        "30 / 64 / 0.1 / 0.001/ 0.1\n",
        "\n",
        "30 / 64 / 0.1 / 0.01/ 0.1\n",
        "\n",
        "30 / 64 / 0.1 / 0.001/ 0.01\n",
        "\n",
        "30 / 64 / 0.01 / 0.001/ 0.1\n",
        "\n",
        "30 / 64 / 0.1 / 0.0001/ 0.01 [95.06%]\n",
        "\n",
        "30 / 100 / 0.1 / 0.001/ 0.1 [94.65%]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL8xhMd6HcQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f7Mi2dNoNWn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}